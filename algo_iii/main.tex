\input{../document.setup}
\usemintedstyle{borland}

\input{custom.commands.tex}
\title{Algoritmos III - Apuntes para final}
\author{Gianfranco Zamboni}

\usepackage[backend=biber,style=chem-acs,sorting=none]{biblatex}
\nocite{*}

\addbibresource{bibliography.bib}

\usemintedstyle[cpp]{bordeland, tabsize=2}
\input{../page.setup}
\setcounter{tocdepth}{3}
	\begin{document}
	\input{tikz.configuration}
	
	\maketitle
	\tableofcontents

\newpage
\section{Introducción}
Un \textbf{algoritmo} es una secuencia \textbf{finita} de pasos  \textbf{precisos} (no deben requerir tomar ninguna decisión subjetiva, ni hacer uso de la intuición o la creatividad) necesarias para llevar a cabo un cálculo de manera correcta.

Se dice que un algoritmo está \textbf{bien definido} cuando toda ejecución del mismo bajo los mismos parámetros devuelve el mismo resultado. Hay que tener en cuenta que si bien esto es lo deseable en la mayoría de los casos, muchas veces es necesario usar variables aleatorias o heurísiticas para conseguir un resultado medianamente decente para problemas que no tienen solución o que son muy dificiles de calcular de manera exacta.

\paragraph{Pseudocódigo:} Es una descripción informal de alto nivel de un algoritmo que transmite el procedimiento a realizar de forma clara y precisa.

\subsection{Analisis de algoritmos}
Cuando tenemos más de un algoritmo para resolver un problema y queremos elegir el \textit{mejor} entre ellos, hay diferentes criterios que podemos analizar para medir la eficiencia de los mismos, según el contexto de aplicación. Los recursos de mayor interés suelen ser el tiempo de cómputo y el espacio de almacenamiento requerido. Podemos considerar dos enfoques:

\paragraph{Análisis empírico:} Implementar el algoritmo en una máquina determinada utilizando un lenguaje determinado, correrlo para un conjunto de instancias y comparar sus tiempos de ejecución. Esto implicaría perder tiempo y esfuerzo programandolo, ejecutandolo y además probarlo con un conjunto de instancias acotados por lo que realmente no podríamos concluir nada sobre su comportomiento con instancias que queden fuera de ese conjunto.

\paragraph{Análisis teórico:} Determinar matemáticamente la cantidad de tiempo que llevará su ejecución como una función de la medidad de la instancia considerada, independinzándolo de la máquina sobre la cuál es implementado el algoritmo y el lenguaje para hacerlo. Para esto es necesario definir un modelo de cómputo, un lenguaje sobre este modelo, cuales son las instancias del problema relevantes al mismo y sus tamaños.

Dicho de otra forma, desde el punto de vista teórico, se busca determinar la velocidad de crecimiento del tiempo o el espacio requerido por el algoritmo en función del tamaño de las instancias del problema.

Dado un problema, se le asigna un entero \(n\) (llamado el \textbf{tamaño} del problema) que servirá como medida de la cantidad de datos de entrada. Y se define como \textbf{complejidad temporal} del algoritmo al tiempo ejecución de un algoritmo expresado como una función que depende de \(n\).

Análogamente, se puede definir la \textbf{complejidad espacial} del algoritmo como el espacio de memoria necesario para su ejecución en función del tamaño del problema.

\subsubsection{Modelo Random Access Machine (RAM)}
Una Random Access Machine (RAM) modela una computadora con un único registro acumulador en la cuál las instrucciones no pueden modificarse. La misma está formada por:

\begin{itemize}
	\item Una \textbf{unidad de entrada} que contiene los datos necesarios para poder ejecutar una corrida del algoritmo. La misma es de solo lectura y, en cada una de sus celdas, puede contener un entero de tamaño arbitrario. Cuando un valor es leído, la cinta se avanza una celda.
	\item Una \textbf{unidad de salida} que es de solo escritrura y se encuentra inicialmente vacía. Cuando una instrucción es ejecutada, se imprime un entero en la celda que se encuentra bajo la lectora y luego se la avanza una posición.
	\item Una \textbf{memoria} que consiste en una secuencia infinita de registros, cada uno de los cuales puede contener un entero de tamaño arbitrario. 
	\item Un \textbf{programa} es una secuencia de instrucciones etiquetadas que no está almacenado en memoria. Asumimos que las instrucciones que contiene un programa, son las instrucciones aritméticas básicas, de entradas/salidas y direccionamiento indirecto y de branching encontradas en la mayoría de las computadoras.
\end{itemize}

Este modelo se usa cuando los tamaños de los problemas a resolver es lo suficientemente pequeño como para que entren en la memoría principal de una computadora o cuando los enteros usados en la computación son los suficientemente chicos como para que entren en una palabra de la misma.

\subsection{Cálculo de complejidad}
Una operación es elemental si su tiempo de ejecución puede ser acotado por una constante dependiente sólo de la implementación particular utilizada. Esta constantes no depende de la medidad de los parámetros de la instancia considerada.

En una máquina RAM, se asume que toda instrucción es una operación elemental con un \textbf{tiempo de ejecución} asociado y definimos el tiempo de ejecución de un programa como:

\[t_A(I) = \text{ suma de los tiempos de ejecución de las instrucciones realizadas por el programa } 
A \text{ con la instancia } I\]

\subsubsection{Tamaño de una instancia}
Para especificar la complejidad en peor caso (tanto espacial como temporal), debemos especificar el tiempo de ejecución requerido por cada instrucción y el espacio de cada registro.

La forma más simple para esto es usar el \textbf{criterio de costo uniforme} en el que cada instrucción requiere una unidad de tiempo y cada registro requiere una unidad de espacio. En general, usaremos este criterio para analizar problemas de ordenamientos o sobre grafos y matrices.


Una segunda opción (un poco más realista), es tomar en cuenta el tamaño real (en bits) de los operandos. Este el \textbf{criterio de costo logarítmico} que toma en cuenta que son necesarios para representar los datos. 
\paragraph{Alfabeto:} Es el conjunto de símbolos que puede interpretar en una máquina. Por ejemplo, las computadoras actuales reconocen el alfabeto binario \(\{0,1\}\).

\vspace*{5mm}
Dada una instancia \(I\), se define \(|I|\) como el número de símbolos de un alfabeto finito necesarios para codificar \(I\). Este tamaño depende del alfabeto elegido.  En el caso de las computadoras actuales, que se manejan con el alfabeto binario, para almacenar un número natural \(n\), se necesitan \(L(n) = \lfloor\log_2{n}\rfloor + 1\) bits.

Usaremos este criterio cuando analicemos problemas sobre números (por ejemplo, el calculo de un factorial).

\subsubsection{Funciones de complejidad}
\paragraph{Complejidad peor caso:} Se toma como complejidad del algoritmo, la función que dada una instancia de tamaño de $n$, devuelve la máxima complejidad posible sobre todas las instancias de ese tamaño.

\paragraph{Complejidad caso promedio:} Se toma como complejidad del algoritmo la función que dada una instancia de tamaño de $n$, devuelve el promedio de todas las complejidades posible sobre todas las instancias de ese tamaño.

\vspace*{5mm}
Dadas dos funciones \(f,g:\nat\to\reales\) decimos que:
\begin{itemize}
	\item \(f(n) = O(g(n))\) si existen \(c\in\reales_+ y n_0\in\nat\) tales que
	\[f(n)\leq cg(n) \text{ para todo } n\geq n_0\]
	\item \(f(n) = \Omega(g(n))\) si existen \(c\in\reales_+ y n_0\in\nat\) tales que
	\[f(n)\geq cg(n) \text{ para todo } n\geq n_0\]
	\item \(f(n) = \Theta(g(n))\) si \[f(n) = O(g(n)) \text{ y } f(n)= \Omega(g(n))\]
\end{itemize}

\subsubsection{Problemas bien resueltos}
Decimos que un problema está \textbf{bien resuelto} si existe un algoritmo de complejidad polinomial que lo resuelva. Es decir, que no consideraremos soluciones sastifactorios a los algoritmos supra-polinomiales.

\subsection{Técnicas de diseño de algoritmos}
\subsubsection{Algoritmos golosos}
Los algoritmos golosos toman decisiones basandose en la información disponible actualemente, sin considerar los efectos que esas decisiones podrían tener en el fúturo. Son fáciles de inventar, implementar y son eficientes. Sin embargo, aunque muchas veces proporcionan heurísticas sencillas para resolver problemas de optimización, no siempre funcionan.

Comunmente, los algoritmos golosos y los problemas que pueden resolver tienen las siguientes carácteristicas:
\begin{itemize}
	\item El problema se debe resolver de alguna manera óptima. Para construir su solución se mantiene \textbf{un conjunto de candidatos} entre los cuales el algoritmo puede elegir para agregar a la solución en el próximo paso. Por ejemplo, un conjunto de monedas o de nodos de un grafos.
	\item A medida que el algoritmo progresa, se acumulan dos conjuntos: Uno contiene candidatos que ya fueron considerados y usados; el otro contiene los candadidatos que fueron consiradosy rechazados.
	\item Hay una \textbf{función de verificación} que dado un conjunto de candidatos verifica si puede ser una solución valida al problema propuesto auque esta no sea la solución óptima.
	\item Hay una \textbf{función de factibilidad} que dado un conjunto de candidatos verifica si se puede extender con nuevos candidatos para conseguir una solución al problema.
	\item Hay una \textbf{función de selección} que decide cual de los candidatos disponibles (que no fueron elegidos ni rechazados) es el más prometedor para hacercanos a la solución deseada.
	\item Hay una f\textbf{función objetivo} que nos da el valor de la solución encontrada. Por ejemplo, el número de monedas utilizadas para el cambio, o la cantidad de nodos usados en un cámino, o cualquier otro valor que estemos intentando optimizar. A diferencia de las otras tres funciones mencionadas anteriormente, esta no aparece explicitamente en el algoritmo goloso.
\end{itemize}

Estructura general de un algoritmo goloso:

\begin{algorithmic}
	\Function{greedy}{\(C\): Set}
	\Comment{\(C\) es el conjunto de candidatos}
	\State \(S\gets\emptyset\)
	\Comment{Conjunto en el que se construye la solución}
	\While{ \(C\neq\emptyset~\land~\lnot\Call{esSolucion}{S}\)}
		\State \(x \gets \Call{seleccionar}{C}\)
		\State \(C \gets C~\backslash~\{x\}\)
		\If{\(\Call{esValido}{S\cup\{x\}}\)}
			\State \(S\gets S\cup\{x\}\)
		\EndIf
	\EndWhile
	\If{\(\Call{esSolucion}{S}\)}
		\Return \(S\)
	\Else
		\State\Return Error: No hay solución
	\EndIf
	\EndFunction
\end{algorithmic}

\subsubsection{Recursividad}
Un algoritmo recursivo se define en términos de si mismo, esto es, en su cuerpo aparece una aplicación suya. En general, las llamadas recursivas se aplican sobre párametros más pequeños que los iniciales. Cuando el parámetro sobre el que se aplica es lo suficientemente chico se ejecuta lo que se llama el \textbf{caso base} que, para su resolución, no necesita llamar a la misma función, terminando así el proceso recursivo.

En general, para calcular la complejidad de un algoritmo recursivo, se debe plantear primero las ecuaciones de recurrencia y luego, utilizando herramientas matemáticas, encontrar la fórmula cerrada (que no dependa de la complejidad de instancias más chicas) de esta ecuación.

\subsubsection{Divide and Conquer}
Es una técnica que consiste en dividir la instancia de un problema a resolver en varias instancias más pequeñas del mismo problema y resolverlas de manera independiente para luego combinar sus soluciones en la solución de la instancia original. 

Para que valga la pena realizar un algoritmo con esta técnica se deben cumplir tres condiciones:

\begin{enumerate}
	\item Debe ser posible descomponer las instancias en subinstancias y combinar las subsoluciones de manera eficiente.
	\item Las subinstancias deben ser todas más o menos del mismo tamaño. La mayoría de los algoritmos de divide and conquer son tales que el tamaño de una subinstancia \(I\), es aproximadamente \(\frac{n}{b}\) donde \(n\) es el tamaño de la instancia original y \(b\) alguna constante.
\end{enumerate}

La estructura general de un algortimo de divide and conquer es la siguiente:

\begin{algorithmic}
	\Function{DivideAndConquer}{\(I\): Instancia del problema}
	\If{\(I\) es suficientemente pequeño o simple}
		\Return \(\texttt{resolver}(I)\)
	\Else
		\State Descomponer \(I\) en subinstancias \(I_1, I_2,\dots, I_k\)
		\For{\(i\gets 1~\textbf{to}~k\)}
			\State \(y_i \gets \Call{DivideAndConquer}{I_i}\)
		\EndFor
		\State Combinar las soluciones \(y_1, y_2,\dots, y_k\) obtenidas para obtener la solución \(y\) de \(I\)
		\State\Return \(y\)
	\EndIf

	\EndFunction
\end{algorithmic}

En este tipo de algoritmos, generalmente las instancias que son caso base toman tiempo constante \(c\) y para las casos recursivos se pueden identificar tres puntos críticos:
\begin{itemize}
	\item Cantidad $r$ de llamadas recursivas que haremos.
	\item Medida de cada subproblema: \(\frac{n}{b}\) para alguna constante \(b\).
	\item Tiempo requerido por el algoritmo para ejecutar, descomponer y combinar para una instancia de tamaño \(n\), \(g(n)\).
\end{itemize}

Entonces, el tiempo total \(T(n)\) consumido por el algoritmo está definido por la siguiente ecuacución de recurrencia:

\[ T(n) = \begin{cases} 
c & \text{ si } n \text{ es caso base }\\
rT\left(\frac{n}{b}\right) + g(n) & \text{ si } n \text{ es caso recursivo } \\
\end{cases}
\]

\subsubsection{Backtracking}
Es una técnica que permite recorrer sistemáticamente todas las posibles configuraciones del espacio de soluciones de un problema computacional. Cuando el algoritmo comienza, no se sabe nada sobre la solución del problema. A medida que va avanzando, se agrega un elemento a la solución para ir consiguiendo soluciones parciales. Si en algún momento, la solución parcial deja de poder ser extendida entonces esa solución se descarta.

La idea básica es tratar de extender una solución parcial del problema hasta, eventualemnte, llegar a obtener una solución completa, que podría ser válida o no. Habitualmente, se utiliza un vector \(a=(a_1,a_2,\dots,a_n)\) para representar una solución candidata donde cada \(a_i\) pertenece a un dominio finito \(A_i\). El espacio de soluciones es el producto cartesiano \(A_1\times\dots\times A_n\).

En síntesis, en cada paso se extienden las soluciones parciales \(a=(a_1,a_2,\dots,a_k)\) con \(k < n\), agregando un elemento más al final. Las nuevas soluciones parciales son sucesoras de la anterior.

Se puede pensar este espacio de búsqueda como un árbol dirigido donde cada vértice representa una solución parcial y un vértice \(x\) es hijo de \(y\) si la solución parcial \(x\) se puede extender desde la solución parcial \(y\). La raíz del árbol se corresponde con el vector vacio.

El proceso de bactkracking recorre este árbol en profundidad. Cuando se puede deducir que una solución parcial no llevará a una solución válida, no es necesario seguir explorando esa rama del árbol y se retrocede hasta encontrar un vértice con un hijo válido por donde seguir la exploración. 

El template general para algoritmos de este tipo es: 

\begin{algorithmic}
	\State \texttt{sol}: Vector \(\gets [] \)
	\State \texttt{encontro}: Booleano \(\gets false \)
	\Function{backtracking}{\(v[1\dots k], s\): Instancia del problema}
	\If{\(k == 0~\land~esSolucion(s)\) }
	\State\(\texttt{sol} \gets s\)
	\State \(\texttt{encontro} \gets true\)
	\Else
	\For{\(i \gets 1 \textbf{ to } k\)}
	%			\State \Call{backtracking}{\(v[1\dots k]\backslash \{v_i\},~s\cup\{v_i\} \)}
	\If{encontro}~\Return
	\EndIf
	\EndFor
	\EndIf
	\EndFunction
\end{algorithmic}

\subsubsection{Programación dinámica}
Esta técnica es aplicada a problemas de optimizacióncombinatoria, donde puede haber muchas soluciones fáctibles, cada una con un valor (o costo asociados) y pretende obtener la solución con mejor valor (o menor costo).

Al igual que dividir y conquistar, se divide al problema en problemas que son más fáciles de resolver. Una vez resueltos estos subproblemas, se combinan las soluciones obtenidas para generar la solución del problema original. Se diferencian, en que esta técnica es iterativa (no recursiva) y es adecuada cuando es necesario resolver varias subisntancias del problema que son iguales ya que se van almacenando los resultados parciales obtenidos para su posterior utilización.

\paragraph{Principio de Optimalidad de Bellman:} Un problema de optimización satisface este principio si en una solución óptima cada subsolución es a su vez óptima del subproblema correspondiente. Es decir, dada una secuencia óptima de decisiones, todas subsecuencia de ella es, a su vez, óptima.

Este principio es condición necesaria del problema para poder usar la técnica. 

\subsubsection{Algoritmos probabilísticos}
Cuando un algoritmo tiene que hacer una elección, a veces es preferible elegir al azar en vez e gastar mucho tiempo tratando de ver cual es la mejor opción.

Una de las características de los algoritmos probabilisticos es que puede comportarse de maneras distintas si se lo usa para computar una misma instancia dos o más veces. Su tiempo de ejecución y resultados pueden variar considerablemente de un uso a otro pudiendo incluso nunca llegar a terminar en ciertas corridas para una instancia específica. Sin embargo, dada la posibilidad de reiniciar el algoritmo y obtener un resultado válido éste es un comportamiento que no molesta. 

Por otro lado, una consecuencia de este comportamiento es que, si hay más de una solución al problema, las mismas se pueden obtener corriendo el algoritmo más de una vez.s

Los algoritmos probabílisticos se pueden clasificar dependiendo de la probabilidad de que devuelvan una respuesta correcta:

\begin{itemize}
	\item \textbf{Algoritmos númericos}: Estos algoritmos devuelven un intervalo de confianza (del estilo ``La solución es \( x \pm y\) con probabilidad \(z\))". Por lo general, mientras más tiempo de proceso les demos, mas preciso es el intervalo que devuelven.
	\item \textbf{Algoritmos de Montecarlo}: Son algoritmo que dan la respuesta exacta con alta probabilidad. Por lo general, no es posible verificar que la respuesta dada sea correcta sin embargo la probabilidad de error disminuye mientras más tiempo se este ejecutando el algoritmo.
	\item \textbf{Algoritmos Las Vegas:} Son algoritmos que siempre dan una respuesta correcta pero puede llegar a no dar ninguna respuesta. 
	\item \textbf{Algoritmos Sherwood:} Son algoritmos que agregan una componente aleatoria a algoritmos determinísticos para tratar de evitar tiempos de ejecución del peor caso. Un ejemplo de esto es el Quicksort con pivote seleccionado aleatoriamente.
\end{itemize}

\subsubsection{Heurísticas}
Dado un problema de optimización \(\sqcap\) díficl de resolver (y para el cual probablemente no exista un algoritmo eficiente), los \textbf{algoritmos heurísticos} definen un procedimiento que intenta conseguir soluciones para el mismo pero puede devolver resultados erróneos o no devolver nada directamente.

En otras palabras: Sea \(I\) es una instancaia del problema y \(x^*(I)\) el valor óptimo de la función a optmizar en dicha instancia. Buscamos crear una heurística \(H\) tal que la solución \(x^H(I)\) devuelta por la misma sea lo más cercano a \(x^*(I)\) posible.

\paragraph{Algoritmos apróximados:} Decimos que \(H\) es un algoritmo \(\epsilon-\)aproximado para el problema \(\sqcap\) si para algún \(\epsilon > 0\) vale que \(|x^H(I) - x^*(I)\leq\epsilon|x^*(I)|\)

\paragraph{Algoritmos con certificados:} Por lo general, si bien los problemas dificiles no pueden ser resueltos con algoritmos polinomiales si se puede verificar que las soluciones provistas por las heurísticas sean correctas con algoritmos polinomiales. 

Se dice que los algoritmos diseñados para realizar esta verificación proveen un certificado que afirma la validez de la soluciones propuestas por una heurística.

\printbibliography[keyword=intro,title={Bibliografía}]

\newpage
\section{Grafos}
\subsection{Definiciones básicas}
Los grafos proporcionan una forma conveniente y flexible de representar problemas de la vida real que consideran una red como estructura subyacente. Esta red puede ser física (como instalaciones eléctticas) o abstractas (que modelan relaciones menos tangibles, como relaciones sociales y bases de datos).

Matemáticamente, un grafo \(G = (V,X)\) es un par de conjuntos, donde \(V\) es un conjunto de \textbf{puntos / nodos / vértices} y \(X\) es un subconjunot del conjunto de pares no ordenados de elementos distintos de \(V\). Los elementos de \(X\) se llamas \textbf{aristas, ejes o arcos}.

\begin{figure}[H]
\begin{center}

	\begin{tikzpicture}
	\node[basicNode] (p) {\(p\)};
	\node[basicNode] (q) [right=of p] {\(q\)};
	\node[basicNode] (r) [below=of p] {\(r\)};
	\node[basicNode] (s) [below=of q] {\(s\)};
	
	\path
	(p) edge (q)
	(p) edge (r)
	(p) edge (s)
	(q) edge (s)
	(r) edge (s)
	;		
	\end{tikzpicture}
\end{center}
	\caption{\(G =([p,~q,~r,~s],~[(p,q),~(p,s),~(q,s),~(r,s)])\)
	}
\end{figure}

Dados \(v\) y \(w \in V\), si \(e=(v,w)\in X\) se dice que \(v\) y \(w\) son \textbf{adyacentes} y que \(e\) es \textbf{incidente} a \(v\) y a \(w\).

La definición de grafo no alcanza para modelar todas las situaciones posibles de una red. Por ejemplo, si se quisiese modelar la ruta de los aviones entre varias ciudades, deberiamos poder modelar varios vuelos entre dos ciudades:
 
\paragraph{Multigrafo:} Es un grafo en el que puede haber varias aristas entre el mismo par de nodos distintos.
		\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
		\node[basicNode] (p) {\(p\)};
		\node[basicNode] (q) [right=of p] {\(q\)};
		\path
		(p) edge[out=10, in=170] (q)
		(p) edge[out=-10, in=-170] (q)
		;		
		\end{tikzpicture}
	\end{center}		
	\caption{Multigrafo}
\end{figure}
\paragraph{Seudografo:} Es un grafo en el que puede haber varias aristas entre cada par de nodos y también puede haber aristas (\textit{loops}) que unan a un nodo con sí mismo.
	
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
		\node[basicNode] (p) {\(p\)};
		\path
		(p) edge[loop left] (p);		
		\end{tikzpicture}
	\end{center}		
	\caption{Multigrafo}
\end{figure}

\paragraph{Notación:} \(n = |V|\) y \(m=|X|\)

\paragraph{Grado:} El grado \(d_G(v)\) de un nodo \(v\) es la cantidad de aristas incidentes a \(v\) en el grafo \(G\). 

Notaremos \(\Delta(G)\) al máximo grado de los vértices de \(G\) y \(\delta(G)\) al mínimo.

\paragraph{Nota:} En un seudografo, un loop aporta 2 al grado del vértice.

\begin{theorem}
La suma de los grados de los nodos de un grafo es igual a dos veces el número de aristas, es decir: \[\sum_{i=1}^{n}v_i = 2m\]
\end{theorem}

\begin{demo}
Sea \(G = (V, X)\) un grafo de \(n\) nodos y \(m\) aristas. Haremos inducción en la cantidad de aristas:

\paragraph{Caso base:} \(m = 1\).

	En este caso, el grafo \(G\) tiene sólo una arista que notamos \(e = (u,w)\). Entonces \(d(u) = d(w) = 1\) y \(d(v) = 0\) para todo \(v\in V\) tal que \(v\neq u,w\). Por lo tanto, \[\sum_{v\in V}d(v) = 2\] y \(2m = 2\), cumpliendose la propiedad.
\paragraph{Paso inductivo:} Consideremos un grafo \(G=(V,X)\) con \(m\) aristas \((m > 1)\). Nuestra hipotesis inductiva es: \textit{En todo grafo \(G' = (V', X')\) con \(m'\) aristas \((m' < m)\), se cumple que } \[\sum_{v\in V'}{d_{G'}}(v) = 2m'\]

Sea \(e = (u,w) \in X\), llamemos \(G'' = (V, X-\{e\})\) al grafo que resulta si le quitamos \(e\) a \(G\). Como la cantidad de aristas de \(G''\) es \(m-1\), \(G''\) cumple con la hipótesis inductiva. Entonces vale que 
 \[\sum_{v\in V}{d_{G''}}(v) = 2(m-1)\]
 
Como \(d_G(u) = d_G''(u) + 1\), \(d_G(w) = d_G''(w) + 1\) y \(d_G(x) = d_G''(x)~\forall~x\in V,~x\neq u,w\), obtenemos que: \[\sum_{v\in V}{d_{G}}(v) = \sum_{v\in V}{d_{G''}}(v) + 2 = 2(m-1) + 2 = 2m~\blacksquare\]
\end{demo}

\begin{coro}
La cantidad de vértices de grado impar de un grafo es par.
\end{coro}

\paragraph{Grafo completo:} Son grafos en los que todos sus vértices son adyacentes entre sí. Notaremos como \(K_n\) al grafo completo de \(n\) vértices.
\begin{center}
\begin{tikzpicture}[graphStyle]
	\node[basicNode] (k11) at (0,0.75) {};
	\node[] (k1Label) [below=of k11, yshift=-0.23cm] {\(K_1\)};

	\node[basicNode] (k21) at (1,0.75) {};
	\node[basicNode] (k22) [right=of k21] {};
	\node[] (k2Label) [below=of k21, yshift=-0.23cm, xshift=0.6cm] {\(K_2\)};
	
	\node[basicNode] (k31) at (3,0.25) {};
	\node[basicNode] (k32) [right=of k31] {};
	\node[basicNode] (k33) [above=of k31, xshift=0.5cm] {};
	\node[] (k3Label) [below=of k31, yshift=0.27cm, xshift=0.6cm] {\(K_3\)};

	\node[basicNode] (k41) at (5,0.25) {};
	\node[basicNode] (k42) [right=of k41] {};
	\node[basicNode] (k43) [above=of k41] {};
	\node[basicNode] (k44) [above=of k42] {};
	\node[] (k4Label) [below=of k41, yshift=0.25cm, xshift=0.6cm] {\(K_4\)};
	
	\node[basicNode] (k51) at (7.5,0) {};
	\node[basicNode] (k52) [right=of k51] {};
	\node[basicNode] (k53) [above right=of k52, yshift=0.25cm, xshift=-0.25cm] {};
	\node[basicNode] (k54) [above left=of k53, xshift=-0.25cm] {};
	\node[basicNode] (k55) [below left=of k54, xshift=-0.25cm] {};
	\node[] (k5Label) [below=of k51, yshift=0.5cm, xshift=0.6cm] {\(K_5\)};
	
	\path
		(k21) edge (k22)
	
		(k31) edge (k32)
		(k31) edge (k33)
		(k32) edge (k33)
		
		(k41) edge (k42)
		(k41) edge (k43)
		(k41) edge (k44)
		(k42) edge (k43)
		(k42) edge (k44)
		(k43) edge (k44)
		
		(k51) edge (k52)
		(k51) edge (k53)
		(k51) edge (k54)
		(k51) edge (k55)
		(k52) edge (k53)
		(k52) edge (k54)
		(k52) edge (k55)
		(k53) edge (k54)
		(k53) edge (k55)
		(k54) edge (k55)
	;
\end{tikzpicture}
\end{center}
\paragraph{Propiedad:} Un grafo completo de \(n\) nodos tiene $\frac{n(n-1)}{2}$ aristas

\paragraph{Grafo complemento:} Dado un grafo \(G=(V,X))\), su grafo complemento \(\bar{G} = (V, \bar{X})\) es el grafo con el mismo conjunto de vértices pero un par de vértices son adyacentes en $\bar{G}$ si y solo si no son adyacentes en $G$.
\begin{center}
	\begin{tikzpicture}[graphStyle]
	\node[basicNode] (v1) at (0,0) {};
	\node[basicNode] (v2) [above=of v1] {};
	\node[basicNode] (v3) [right=of v1] {};
	\node[basicNode] (v4) [right=of v2] {};
	\node[basicNode] (v5) [above right=of v3, yshift=-0.12cm] {};
	\node[] (glabel) [below right=of v1, xshift=-0.5cm] {Grafo \(G\)};
	
	\node[basicNode] (bv1) at (3,0) {};
	\node[basicNode] (bv2) [above=of bv1] {};
	\node[basicNode] (bv3) [right=of bv1] {};
	\node[basicNode] (bv4) [right=of bv2] {};
	\node[basicNode] (bv5) [above right=of bv3, yshift=-0.12cm] {};
	\node[] (bglabel) [below right=of bv1, xshift=-0.5cm] {Grafo \(\bar{G}\)};
	
	\path
	(v1) edge (v2)
	(v1) edge (v3)
	(v2) edge (v3)
	(v2) edge (v4)
	(v3) edge (v5)
	(v4) edge (v5)
	
	(bv1) edge (bv4)
	(bv1) edge[out=270,in=270] (bv5)
	(bv2) edge[in=90] (bv5)
	(bv3) edge (bv4)
	;;
	\end{tikzpicture}
\end{center}

\paragraph{Propiedad:} Si \(G\) tiene \(n\) vértices y \(m\) aristas, entonces: \[m_{\bar{G}} = \frac{n(n-1)}{2} - m\]

\subsubsection{Caminos y ciclos}
\paragraph{Camino:} Un camino en un grafo, es una secuencia alternada de vértices y aristas \(P = v_0e_1v_1e_2\dots v_{k-1}e_kv_k\) tal que un exremo de la arista \(e_i\) es \(v_{i-1}\) y el otro es \(v_i\) para \(i=1\dots k\).

\paragraph{Camino simple:} Es un camino que no pasa dos veces por el mismo vértice.

\paragraph{Sección:} La sección de un camino \(P = v_0e_1v_1e_2\dots v_{k-1}e_kv_k\) es una subsecuencia \(v_ie_{i+1}v_{i+1}e_{i+2}\dots v_{j-1}e_jv_j\) de términos consecutivos de \(P\), y lo notamos como \(P_{v_iv_j}\).

\paragraph{Circuito:} Es un camino que empieza y termina en el mismo vértice.

\paragraph{Circuito Simple:} Es un circuito de tres o más vértices que no pasa dos veces por el mismo vértices.

\begin{center}
	\begin{tikzpicture}[graphStyle]
	\node[basicNode] (v1) at (0,0) {\(v_1\)};
	\node[basicNode] (v2) [above=of v1] {\(v_2\)};
	\node[basicNode] (v3) [right=of v1] {\(v_3\)};
	\node[basicNode] (v4) [right=of v2] {\(v_4\)};
	\node[basicNode] (v5) [above right=of v3, yshift=-0.5cm] {\(v_5\)};
	\node[] (glabel) [below right=of v1, xshift=-1cm, text width=3cm] {Camino no simple \(P = v_2v_3v_1v_2v_4\)};

	\node[basicNode] (bv1) at (5,0) {\(v_1\)};
	\node[basicNode] (bv2) [above=of bv1] {\(v_2\)};
	\node[basicNode] (bv3) [right=of bv1] {\(v_3\)};
	\node[basicNode] (bv4) [right=of bv2] {\(v_4\)};
	\node[basicNode] (bv5) [above right=of bv3, yshift=-0.5cm] {\(v_5\)};
	\node[] (bglabel) [below right=of bv1, xshift=-1cm, text width=3cm] {Camino simple \(P = v_1v_2v_5v_4\)};	

	\node[basicNode] (cv1) at (0,-5) {\(v_1\)};
	\node[basicNode] (cv2) [above=of cv1] {\(v_2\)};
	\node[basicNode] (cv3) [right=of cv1] {\(v_3\)};
	\node[basicNode] (cv4) [right=of cv2] {\(v_4\)};
	\node[basicNode] (cv5) [above right=of cv3, yshift=-0.5cm] {\(v_5\)};
	\node[] (cglabel) [below right=of cv1, xshift=-1.25cm, text width=3.5cm] {Circuito no simple \(P = v_1v_3v_2v_4v_5v_2v_1\)};	

	\node[basicNode] (dv1) at (5,-5) {\(v_1\)};
	\node[basicNode] (dv2) [above=of dv1] {\(v_2\)};
	\node[basicNode] (dv3) [right=of dv1] {\(v_3\)};
	\node[basicNode] (dv4) [right=of dv2] {\(v_4\)};
	\node[basicNode] (dv5) [above right=of dv3, yshift=-0.5cm] {\(v_5\)};
	\node[] (dglabel) [below right=of dv1, xshift=-1cm, text width=3cm] {Circuito simple \(P = v_2v_3v_5v_4v_2\)};
	
	\path
	(v1) edge[red, line width=0.75mm]  (v2)
	(v1) edge[red, line width=0.75mm]  (v3)
	(v2) edge[red, line width=0.75mm] (v3)
	(v2) edge[red, line width=0.75mm]  (v4)
	(v2) edge (v5)
	(v3) edge (v5)
	(v4) edge (v5)
	
	(bv1) edge[red, line width=0.75mm]   (bv2)
	(bv1) edge (bv3)
	(bv2) edge (bv3)
	(bv2) edge (bv4)
	(bv2) edge[red, line width=0.75mm]   (bv5)
	(bv3) edge (bv5)
	(bv4) edge[red, line width=0.75mm]   (bv5)
	
	(cv1) edge[red, line width=0.75mm] (cv2)
	(cv1) edge[red, line width=0.75mm] (cv3)
	(cv2) edge[red, line width=0.75mm] (cv3)
	(cv2) edge[red, line width=0.75mm] (cv4)
	(cv2) edge[red, line width=0.75mm] (cv5)
	(cv3) edge (cv5)
	(cv4) edge[red, line width=0.75mm] (cv5)
	
	(dv1) edge (dv2)
	(dv1) edge (dv3)
	(dv2) edge[red, line width=0.75mm] (dv3)
	(dv2) edge[red, line width=0.75mm] (dv4)
	(dv2) edge (dv5)
	(dv3) edge[red, line width=0.75mm] (dv5)
	(dv4) edge[red, line width=0.75mm] (dv5)
	;
	\end{tikzpicture}
\end{center}

\paragraph{Longitud de un camino:} Dado un camino \(P\), su longitud \(l(P)\) es la cantidad de aristas que tiene.

\paragraph{Distancia:} La distancia \(d(v,w)\) entre dos vértices \(v\) y \(w\) se define como la longitud del camino más corto entre \(v\) y \(w\). 
\begin{itemize}
	\item Si no existe camino entre \(v\) y \(w\) decimos que \(d(v,w) = \infty\).
	\item \(\forall~v\in V,~d(v,v) = 0\)
\end{itemize}

\begin{proposicion}
	Si un camino \(P\) entre \(v\) y \(w\) tiene longitud \(d(v,w)\) entonces \(P\) es un camino simple.
\end{proposicion}
\begin{demo}
	Demostración por el absurdo. Sea \(P = v\dots w\) un camino entre \(v\) y \(w\) con \(l(P) = d(v,w)\). Supongamos que \(P\) no es simple, es decir existe un vértice \(u\) que se repite en \(P\) (\(u\) podría llegar a ser \(v\) o \(w\)) entonces \(P = v\dots u \dots u \dots w\).
	
	Formemos ahora un camino \(P' = P_{vu}P_{uw}\), como \(P'\) no tiene todos los nodos que están en \(P_{uu}\) nos queda que \(l(P') < l(P) = d(v,w)\). Esto genera un absurdo porque por definición \(d(v,w)\) es la longitud del camino más corto entre \(v\) y \(w\). \(\blacksquare\)
\end{demo}

\begin{proposicion}
	La función de distancia cumple las siguientes propiedades para todo \(u,v,w\) pertenecientes a \(V\):
	\begin{enumerate}
		\item \(d(u,v)\geq 0\)
		\item \(d(u,v)=0 \iff u=v\)
		\item \(d(u,v) = d(v,u)\)
		\item \(d(u,w) \leq d(u,v) + d(v,w)\)
	\end{enumerate}
\end{proposicion}

\subsubsection{Subgrafos y Componentes Conexas}

\paragraph{Subgrafo:}
 Dado un grafo \(G=(V_G, X_G)\), un subgrafo de \(G\) es un grafo \(H = (V_H, X_H)\) tal que \(V_H\subseteq V_G\) y \(X_H\subseteq X_G\cap (V_H\times V_H)\). Y notamos \(H\subseteq G\).

\begin{itemize}
	\item Si \(H \subseteq G\) y \(H\neq G\), entonces \(H\) es un \textbf{subgrafo propio} de \(G\) y notamos \(H \subset G\).
	\item \(H\) es un subgrafo generador de \(G\) si \(H \subseteq G\) y \(V_G = V_H\).
	\item Un subgrafo \(H=(V_H, X_H)\) de \(G=(V_G, X_G)\), es un \textbf{subgrafo inducido} si \(\forall~u,v\in V_H\) tal que \((u,v)\in X_G \Rightarrow (u,v)\in X_H\).
	\item Un subgrafo inducido de \(G=(V_G, X_G)\) por un conjunto de vértices \(V'\subseteq V_G\), se denota como \(G_{[V']}\).
\end{itemize}

\begin{center}
	\begin{tikzpicture}[graphStyle]
	\node[basicNode] (v1) at (0,0) {};
	\node[basicNode] (v2) [above left=of v1] {};
	\node[basicNode] (v3) [above right=of v1] {};
	\node[] (glabel) [below=of v1] {Grafo \(G\)};
	
	\node[basicNode] (bv1) at (3,0) {};
	\node[basicNode] (bv2) [above left=of bv1] {};
	\node[] (glabel) [below=of bv1, text width=2cm, text centered] {Sugrafo propio de \(G\)};
	
	\node[basicNode] (cv1) at (6,0) {};
	\node[basicNode] (cv2) [above left=of cv1] {};
	\node[basicNode] (cv3) [above right=of cv1] {};
	\node[] (glabel) [below=of cv1, text width=3cm, text centered] {Grafo que no es subgrafo de \(G\)};
	
	\node[basicNode] (dv1) at (9,0) {};
	\node[basicNode] (dv2) [above left=of dv1] {};
	\node[] (glabel) [below=of dv1, text width=2.5cm, text centered] {Subgrafo inducido de \(G\)};
	
	\path
		(v1) edge (v2)
		(v1) edge (v3)
				
		(cv2) edge (cv3)
		
		(dv1) edge (dv2)
	;
	\end{tikzpicture}
\end{center}
\paragraph{Grafo conexo:}
Un grafo se dice \textbf{conexo} si existe camino entre todo par de vértices.

\paragraph{Componente conexa:} Una componente conexa de un grafo \(G=(V_G, X_G)\) es un subgrafo conexo maximal de \(G\). Esto es un subgrafo \(H = (V_H, X_H)\) inducido de \(G\) tal que \(H\) es conexo y si tratamos de agregar cualquier vértice \(v\in V_G \backslash V_H\) entonces nos queda un grafo no conexo.


\begin{center}
	\begin{tikzpicture}[graphStyle]
	\node[basicNode] (v1) at (0,0) {};
	\node[basicNode] (v2) [above left=of v1] {};
	\node[basicNode] (v3) [above right=of v1] {};
	\node[basicNode] (v4) [right=of v3] {};
	\node[basicNode] (v5) [below=of v4] {};
	\node[] (glabel) [below=of v1, xshift=1cm] {Grafo conexo.};


	\node[basicNode] (bv1) at (5,0) {};
	\node[basicNode] (bv2) [above left=of bv1] {};
	\node[basicNode] (bv3) [above right=of bv1] {};
	\node[basicNode, fill=red] (bv4) [right=of bv3] {};
	\node[basicNode, fill=red] (bv5) [below=of bv4] {};
	\node[] (glabel) [below=of bv1, text width=4cm, xshift=1cm, text centered] {Grafo no conexo. Tiene dos componentes conexas (la verde y la roja)};
	
	\path
	(v1) edge (v2)
	(v1) edge (v3)
	(v3) edge (v4)
	(v4) edge (v5)
	
	(bv1) edge (bv2)
	(bv1) edge (bv3)
	(bv4) edge (bv5)
	;
	\end{tikzpicture}
\end{center}

\newpage
\subsection{Grafos bipartitos}
Un grafo \(G = (V,X)\) se dice \textbf{bipartito} si existe una partición \(V_1,V_2\) del conjunto de vértices \(V\) tal que:
\begin{itemize}
	\item \(V = V_1\cup V_2\)
	\item \(V_1\cap V_2 \neq\emptyset\)
	\item \(V_1\neq\emptyset\)
	\item \(V_2\neq\emptyset\)
	\item Todas las aristas de \(G\) tiene un extremo en \(V_1\) y otro en \(V_2\)
\end{itemize}

Un grafo bipartito con partición \(V_1,~V_2\) es \textbf{bipartito completo} si todo vértice en \(V_1\) es adyacente a todo vértice en \(V_2\).

\begin{center}
	\begin{tikzpicture}[graphStyle]
	\node[basicNode] (v1) at (0,0) {};
	\node[basicNode] (v2) [above left=of v1] {};
	\node[basicNode] (v3) [above right=of v1] {};
	\node[] (glabel) [below=of v1] {Grafo no bipartito.};
	
	\node[basicNode, fill=red] (bv1) at (4,0) {};
	\node[basicNode] (bv2) [above=of bv1] {};
	\node[basicNode,fill=red] (bv3) [right=of bv2] {};
	\node[basicNode] (bv4) [below=of bv3] {};
	\node[basicNode] (bv5) [below left=of bv1] {};
	\node[basicNode, fill=red] (bv6) [above left=of bv2] {};
	\node[basicNode] (bv7) [above right=of bv3] {};
	\node[basicNode, fill=red] (bv8) [below right=of bv4] {};
	\node[] (bglabel) [below right=of bv5, xshift=-0.5cm] {Grafo bipartito.};
	
	\node[basicNode] (cv1) at (8,-0.5) {};
	\node[basicNode] (cv2) [right=of cv1] {};
	\node[basicNode] (cv3) [right=of cv2] {};
	\node[basicNode] (cv4) [right=of cv3] {};
	\node[basicNode, fill=red] (cv5) [above=of cv1, yshift=1cm] {};
	\node[basicNode, fill=red] (cv6) [right=of cv5, xshift=.5cm] {};
	\node[basicNode,fill=red] (cv7) [above=of cv4, yshift=1cm] {};
	\node[] (cglabel) [below right=of cv1, xshift=-1cm] {Grafo bipartito completo.};
	\path
	(v1) edge (v2)
	(v1) edge (v3)
	(v3) edge (v2)

	(bv1) edge (bv2)	
	(bv1) edge (bv4)
	(bv1) edge (bv5)
	(bv2) edge (bv3)	
	(bv2) edge (bv6)
	(bv3) edge (bv4)	
	(bv3) edge (bv7)
	(bv4) edge (bv8)	
	(bv5) edge (bv6)
	(bv5) edge (bv8)
	(bv6) edge (bv7)
	(bv7) edge (bv8)
	
	(cv1) edge (cv5)
	(cv1) edge (cv6)
	(cv1) edge (cv7)
	(cv2) edge (cv5)
	(cv2) edge (cv6)
	(cv2) edge (cv7)
	(cv3) edge (cv5)
	(cv3) edge (cv6)
	(cv3) edge (cv7)
	(cv4) edge (cv5)
	(cv4) edge (cv6)
	(cv4) edge (cv7)
	;
	\end{tikzpicture}
\end{center}

\begin{theorem}
	Un grafo \(G\) con dos o más vértices es bipartito si y solo si no tiene circuitos de longitud impar.
\end{theorem}
\begin{demo}
	Como un grafo es bipartito si y solo si cada una de sus componentes conexas es bipartita alcanza con demostrar el teorema para grafos conexos. 
	\paragraph{\(\left.\Rightarrow\right) \)} Sea \(G\) un grafo conexo bipartito y \(V = (V_1,V_2)\) su bipartición.
	
	Si \(G\) no tiene circuitos entonces el teorema se cumple de manera trivial.
	
	Supongamos que \(G\) tiene circuitos y sea \(C = v_1v_2\dots v_kv_1\) un circuito de \(G\). Sin perdida de generalidad, supongamos que \(v_1\in V_1\). Como \((v_1,v_2) \in X\) entonces \(v_2\in V_2\)). En general, \(v_{2i + 1}\in V_1\) y \(v_{2i}\in V_2\). Como \(v_1\in V_1\) y \((v_k,v_1)\in X\), debe pasar \(v_k\in V_2\). Luego \(k = 2i\) para algún \(i\), lo que implica que \(l(C)\) es par. \(\blacksquare\)
\end{demo}
\begin{demoPart}
	
	\paragraph{\(\left.\Leftarrow\right)\)} Sea \(G\) un grafo conexo sin circuitos impares. Sea \(u\) cualquier vértice de \(V\). Definimos: \[V_1 = \{ v\in V~/~d(u,v) \text{ es par}\}\] \[V_2 = \{ v\in V~/~d(u,v) \text{ es impar}\}\]

	\(V_1\) y \(V_2\) definen una partición de \(V\) (ya que como \(G\) es conexo no hay vértices a distancia \(\infty\) de \(v\)). Tenemos que ver que definen una bipartición, es decir que no existe arista entre dos vértice de \(V_1\) y dos de \(V_2\). Hagamos esto por el absurdo:
	
	Supongamos que no es bipartición, es decir existen \(v,w\in V_1\) tales que \((v,w)\in X\). Si \(v = u\), entonces \(d(v,w) = 1\), pero esto no puede pasar por que \(d(u,w) \) es par. Lo mismo para \(w\), luego \(v\neq u,w\).
	
	Sea \(P\) un camino mínimo entre \(v\) y \(u\) y \(Q\) un camino mínimo entre \(v\) y \(w\). Como \(u,w\in V_1\), \(P\) y \(Q\) tienen longitud par.
	
	Sea \(z\in V\) el último nodo en el que \(P\) y \(Q\) se cruzan (podría pasar que \(z = u\)). Como \(P\) y \(Q\) definen las distancias a \(v\) y \(w\) respectivamente desde \(u\), entonces \(P_{uz}\) y \(Q_{uz}\) tienen que ser caminos mínimos. Osea que \(l(P_{uz}) = l(Q_{uz}) = d(u,z)\).
	
	Entonces \(l(P_{zv})\) y \(l(Q_{zw})\) tienen igual paridad. Definamos \(C = P_{zv}(v,w)Q_{wz}\), entonces \(l(C) = l(P_{zv}) + l(Q_{wz}) + 1\) que es una longitud impar. Absurdo, partiamos de la supocisión de que \(G\) no tenia circuitos de longitud impar \(\blacksquare\)
\end{demoPart}

\subsection{Representación de grafos}
\subsubsection{Matriz de adyacencia de un grafo}
Dado un grafo \(G\), se define su \textbf{matriz de adyacencia} \(A\in\reales^{n\times n}\), \(A = [a_{ij}]\) como:

\[a_{ij} = \begin{cases} 
1 & \text{ si } G \text{ tiene una arista entre } v_i \text{ y } v_j\\
0 & \text{ si no }
\end{cases}
\]

\begin{proposicion}
	Si \(A\) es la matriz de adyacencia del grafo \(G\), entonces:
	\begin{itemize}
		\item La suma de los elementos de la columna (o fila) \(i\) de \(A\) es igual a \(d(v_i)\).
		\item Los elementos de la diagonal de \(A^2\) indican los grados de los vértices: \(a_{ii}^2 = d(v_i)\).
	\end{itemize}
\end{proposicion}

Para los seudografos, se generaliza la definición dada de la seguiente manera:

\[a_{ij} = \begin{cases} 
\text{ cantidad de aristas }(v_i,v_j)& \text{ si } i \neq j\\
\text{ cantidad de loops sobre } v_i & \text{ si } i = j
\end{cases}
\]

\subsubsection{Matriz de incidencia de un grafo}
Dado un grafo \(G\), se define su \textbf{matriz de incidencia} \(B\in\reales^{m\times n}\) con \(B = [b_{ij}]\) como:

\[b_{ij} = \begin{cases}
1 & \text{ si la arista } i \text{ es incidente al vértice } v_j \\
0 & \text{ sino }
\end{cases}\]

\begin{proposicion}
	Si \(B\) es la matriz de incidencia del grafo \(G\), entonces:
	\begin{itemize}
		\item La suma de los elementos de cada fila es igual a 2.
		\item La suma de los elementos de la \(j\)-ésima columna es igual a \(d(v_j)\).
	\end{itemize}
\end{proposicion}

Para los pseudografos, se generaliza la definición de la siguiente forma:

\[b_{ij} = \begin{cases}
2 & \text{ si la arista } i \text{ es loop sobre el vertice } v_j \\
1 & \text{ si la arista } i \text{ no es loop e incide sobre el vértice } v_j \\
0 & \text{ sino }
\end{cases}\]

\subsection{Digrafos}
\paragraph{Grafo dirigido o digrafo:} Es un par de conjuntos \(G = (V,X)\) donde \(V\) es el conjunto de nodos y \(X\) es un subconjunto del conjunto de pares \textbf{ordenados} de elementos distintos de \(V\). A los elementos de \(X\) los llamaremos \textbf{arcos}.

Dado un arco \(e=(u,w)\) llamaremos al primer elemento (\(u\)) \textbf{cola} de \(e\) y al segundo (\(w\)), \textbf{cabeza} de \(e\).

\paragraph{Grado de entrada:} \(d_{in}(v)\) de un vértice \(v\) de un digrafo es la cantidad de arcos que llegan a \(v\). Es decir, la cantidad de arcos que tienen como cabeza a \(v\).

\paragraph{Grado de entrada:} \(d_{out}(v)\) de un nodo \(v\) de un digrafo es la cantidad de arcos que salen de \(v\). Es decir, la cantidad de arcos que tiene a \(v\) como cola.

\paragraph{Grafo subyacente:} El grafo subyacente de un digrafo \(G\) es el grafo \(G^S\) que resulta de remover las direcciones de sus arcos (si para un par de vértices hay arcos en ambas direcciones, sólo se coloca una arista entre ellos).

\paragraph{Matriz de adyacencia:} de un digrafo \(G\), \(A\in\reales^{n\times n}\), \(A = [a_{ij}]\) se define como:

\[a_ij = \begin{cases}
1 & \text{ si } G \text{ tiene un arco } v_i a v_j \\
0 & \text{ si no } \\
\end{cases}\]

\begin{proposicion}
	Si \(A\) es la matriz de adyacencia del digrafo \(G\), entonces:
	\begin{itemize}
		\item La suma de los elementos de la fila \(i\) de \(A\) es igual a \(d_{out}(v_i)\).
		\item La suma de los elementos de la columna \(i\) de \(A\) es igual a \(d_{in}(v_i)\).
	\end{itemize}
\end{proposicion}

\paragraph{Matriz de incidencia:} de un digrafo \(G\), \(B\in\reales^{m\times n}\), \(B = [b_{ij}]\) se define como:

\[b_ij = \begin{cases}
1 & \text{ si } v_j \text{ es cabeza del arco } i \\
-1 & \text{ si } v_j \text{ es cola del arco } i \\
0 & \text{ si no } \\
\end{cases}\]

\begin{proposicion}
	Si \(B\) es la matriz de incidencia del digrafo \(G\), entonces la suma de los elementos de cada fila es igual a cero.
\end{proposicion}

\paragraph{Camino orientado:} Es una sucesión de arcos \(e_1e_2\dots e_k\) tal que el primer elemento del arco \(e_i\) coincide con el segundo de \(e_{i-1}\) y el segundo elemento de \(e_i\) con el primero de \(e_{i+1}\) con \(i = 2,\dots,k-1\)

\paragraph{Digrafo fuertemente conexo:} Es un digrafo \(G\) tal que para todo par de vértices \(u,v\in 	V_G\) existe un camino orientado de \(u\) a \(v\).

\newpage
\section{{Árboles}}
\paragraph{Árbol:} Grafo conexo sin circuitos simples.

\paragraph{Puente:} Una arista \(e\) de un grafo \(G\) tal que \(G - e\) tiene más componentes conexas que \(G\).

\begin{lema}
	La unión de dos caminos simples distintos entre dos vértices contiene un circuito simple.
\end{lema}

\begin{lema}
	Sea \(G =(V, X)\) un grafo conexo y \(e\in X\). \(G-e = (V, X\backslash\{e\})\) es conexo si y solo si \(e\) pertenece a un circuito simple de \(G\).
	
	En otras palabras: Una arista \(e\in X\) es puente si y solo si \(e\) no pertence a ningún circuito simple de \(G\)
\end{lema}

\begin{demo} 
	\paragraph{\(\Rightarrow\))} Sea \(e =(u,v)\in X\). Por hipotesis \(G-e\) es conexo. Entonces existe un camino simple \(P_{uw}\) entre \(u\) y \(w\) en \(G - e\) (que no usa a \(e\)). Luego podemos definir a \(C = P_{uw} + e\) como un circuito simple de \(G\) que contiene a \(e\).
	
	\paragraph{\(\Leftarrow\) )} Sea \(C\) un circuito simple de \(G\) que contiene a \(e = (u,w)\). Entonces podemos partir a \(C\) en la arista \(e\) y un camino simple \(P_{uw})\) entre \(u\) y \(w\) (que no contiene a \(e\)).

	Como \(G\) es conexo, hay camino entre todo par de vértices. Si esos caminos no una a \(e\), entonces siguen estando en \(G-e\).
	
	Si un camino de \(G\) usa \(e\), en \(G-e\) hay un camino alternativo que es cambiando a \(e\) por \(P_{uw}\) en \(Q\). Entonces sigue habiendo camino entre todo par de vértices en \(G-e\). Osea \(G-e\) es conexo.
\end{demo}
\begin{theorem}
	Dado un grafo \(G=(V,X)\), son equivalentes:
	\begin{enumerate}
		\item \(G\) es un árbol.
		\item \(G\) es un grafo sin circuitos simples y para toda arista \(e\) tal que \(e\notin X\), \(G+e = (V, X\cup\{e\})\) tiene exactamente un circuito simple. Además, ese circuito contiene a \(e\).
		\item Existe exactamente un camino simple entre todo par de vértices.
		\item \(G\) es conexo pero si se quita cualquier arista a \(G\) queda un grafo no conexo (toda arista es puente).
	\end{enumerate}
\end{theorem}

\newpage
\appendix
\section{Hoja de complejidades}
\begin{center}
\begin{tabular}{|l|c|}
	\hline
	\textbf{Algoritmo} & \textbf{Complejidad} \\
	\hline
	\multicolumn{2}{|c|}{\cellcolor{blue!25}\textbf{De Búsqueda}}\\
	\hline
	Secuencial & \(O(n)\) \\
	\hline
	Binaria & \(O(\log{n})\) \\
	\hline
	\multicolumn{2}{|c|}{\cellcolor{blue!25}\textbf{De Ordenamiento}}\\
	\hline
	Bubblesort & \(O(n^2)\) \\
	\hline
	Quicksort & \(O(n^2)\) \\
	\hline
	Heapsort & \(O(n\log{n})\)* \\
	\hline
\end{tabular}
\end{center}

\paragraph{*} \(O(n\log{n})\) es la complejidad óptima para algoritmos de ordenamiento basados en comparaciones. 

\color{red}

Grafos Definiciones básicas: isomorfismos. Arboles: caracterización, árboles orientados, árbol generador. Enumeración. Grafos eulerianos y hamiltonianos. Planaridad. Coloreo. Número cromático. Matching, conjunto independiente, recubrimiento. Recubrimiento de aristas y vértices.



Algoritmos en grafos y aplicaciones Representación de un grafo en la computadora: matrices de incidencia y adyacencia, listas. Algoritmos de búsqueda en grafos: BFS, DFS, A*. Mínimo árbol generador, algoritmos de Prim y Kruskal. Arboles ordenados: códigos unívocamente descifrables. Algoritmos para detección de circuitos. Algoritmos para encontrar el camino mínimo en un grafo: Dijkstra, Ford, Dantzig. Planificación de procesos: PERT/CPM. Algoritmos heurísticos: ejemplos. Nociones de evaluación de heurísticas y de técnicas metaheurísticas. Algoritmos aproximados. Heurísticas para el problema del viajante de comercio. Algoritmos para detectar planaridad. Algoritmos para coloreo de grafos. Algoritmos para encontrar el flujo máximo en una red: Ford y Fulkerson. Matching: algoritmos para correspondencias máximas en grafos bipartitos. Otras aplicaciones.



Problemas NP-completos Problemas tratables e intratables. Problemas de decisión. P y NP. Maquinas de Turing no determinísticas. Problemas NP-completos. Relación entre P y NP. Problemas de grafos NP-completos: coloreo de grafos, grafos hamiltonianos, recubrimiento mínimo de las aristas, corte máximo, etc.
\end{document}

