\section{Congestión}
\subsubsection*{Resource allocation} Es el proceso por el cual los elementos de la red tratan de satisfacer las demandas competitivas que las aplicaciones tienen para los recursos de la red, principalmente el ancho de banda del enlace y el espacio de buffer en los routers o switches.

\subsubsection*{control de congestión} 
Utilizamos el término control de congestión para describir los esfuerzos realizados por los nodos de la red para prevenir o responder a las condiciones de sobrecarga. Dado que la congestión es generalmente mala para todos, el primer orden de negocios es hacer que la congestión disminuya, o prevenirla en primer lugar. Esto podría lograrse simplemente persuadiendo a algunos hosts a dejar de enviar, mejorando así la situación para todos los demás. Sin embargo, es más común que los mecanismos de control de congestión tengan algún aspecto de equidad, es decir, tratan de compartir el dolor entre todos los usuarios, en lugar de causar un gran dolor a unos pocos. 

\subsubsection*{Flujo} 
Para esta sección vamos a asumir que la red es esencialmente sin conexión, con cualquier servicio orientado a la conexión implementado en el protocolo de transporte que se ejecuta en los hosts finales. Sin embargo, generalmente ocurre que una secuencia de datagramas entre un par de hosts particulares \textbf{fluye} a través de un conjunto particular de enrutadores,  La suposición de que todos los datagramas son completamente independientes en una red sin conexión es demasiado fuerte. Los datagramas ciertamente se conmutan independientemente, pero generalmente ocurre que una secuencia de datagramas entre un par de hosts particulares fluye a través de un conjunto particular de enrutadores. Esta idea de un flujo, una secuencia de paquetes enviados entre un par \(\langle origen , destino\rangle\) y siguiendo la misma ruta a través de la red, es una abstracción importante en el contexto de la asignación de recursos.

\subsubsection*{Soft state}
Dado que varios paquetes relacionados fluyen a través de cada enrutador, a veces tiene sentido mantener información de estado para cada flujo, información que se puede utilizar para tomar decisiones de asignación de recursos sobre los paquetes que pertenecen al mismo. Este estado a veces se llama \textbf{soft state}. Este estado representa un punto intermedio entre una red puramente sin conexión que no mantiene ningún estado en los enrutadores y una red puramente orientada a la conexión que mantiene un \textbf{hard state} en los enrutadores. En general, el funcionamiento correcto de la red no depende de que el \textbf{soft state} esté presente (cada paquete aún se enruta correctamente sin tener en cuenta este estado).

Un flujo puede definirse implícitamente o establecerse explícitamente. En el primer caso, cada enrutador observa los paquetes que viajan entre el mismo par de \(\langle origen , destino\rangle\) inspeccionando las direcciones en el encabezado y trata estos paquetes como pertenecientes al mismo flujo. En el último caso, la fuente envía un mensaje de configuración de flujo a través de la red, declarando que un flujo de paquetes está a punto de comenzar.

\subsubsection*{Quality of Service (QoS)} 
Con \textbf{best effort serivce}, todos los paquetes reciben el mismo tratamiento, sin que los hosts finales tengan la oportunidad de pedir a la red que algunos paquetes o flujos reciban ciertas garantías o un servicio preferencial. Hay modelos de servicios que proporciona múltiples calidades de servicio (QoS). 

\subsection{Asiganción de recursos}
\subsubsection{Caracteristicas de mecanismos de asignación de recursos}
\subsubsection*{Router Centric vs Host Centric}
Los mecanismos de asignación de recursos se pueden clasificar en dos grupos amplios: aquellos que abordan el problema desde dentro de la red y aquellos que lo abordan desde los bordes: 

En un diseño centrado en el enrutador, cada enrutador se hace responsable de decidir cuándo se reenvían los paquetes y seleccionar qué paquetes se deben descartar, así como de informar a los hosts que generan el tráfico de red cuántos paquetes se les permite enviar. En un diseño centrado en el host, los hosts finales observan las condiciones de la red y ajustan su comportamiento en consecuencia. Estos dos grupos no son mutuamente excluyentes, dado que es el caso de que tanto los enrutadores dentro de la red como los hosts en los bordes de la red participan en la asignación de recursos, el problema real es dónde recae la mayor parte de la carga.

\subsubsection*{Reserva vs Feedback}
Los mecanimos de resource allocation según si usan reservas o feedback.

En un sistema basado en reservas, alguna entidad le pide a la red una cierta cantidad de capacidad para asignar a un flujo. Cada router asigna entonces suficientes recursos (buffers y / o porcentaje del ancho de banda al enlace) para satisfacer esta solicitud. Si hay un router que no puede satisfacer el pedio porque eso comprometería sus recursos, entonces rechaza la reserva. En un enfoque basado en feedback, los hosts finales comienzan a enviar datos sin reservar primero ninguna capacidad y luego ajustan su velocidad de envío de acuerdo con el feedback que reciben. Esta retroalimentación puede ser explícita (es decir, un enrutador congestionado envía un mensaje pidiendole que reduzca la velocidad) o implícita (es decir, ajusta su velocidad de envío de acuerdo con el comportamiento observable de la red, como pérdidas de paquetes).

\subsubsection*{Window vs Rate}
Una tercera forma de caracterizar estos mecánismos es según si son basados en ventanas o en tasas (rate). TCP es uno de los protocolos basados en ventana en los que el receptor anuncia una ventana al remitente. Esta ventana corresponde a la cantidad de espacio de búfer que tiene el receptor y limita la cantidad de datos que el remitente puede transmitir; es decir, admite el control de flujo. Este mecanismo, tambien se puede usar dentro de la red para reservar espacio de búfer. También es posible controlar el comportamiento de un remitente utilizando un rate, es decir, cuántos bits por segundo el receptor o la red pueden absorber. El control basado en rates tiene sentido para muchas aplicaciones multimedia, que tienden a generar datos a una velocidad promedio y que necesitan al menos un rendimiento mínimo para ser útiles

\subsubsection{Criterios de evaluación}
Para calcular la efectividad de un esquema de asignación de recursos, se consideran las dos métricas principales de la red: throughput y delay. Queremos la mayor cantidad de throughput y la menor cantidad de delay posible. Desafortunadamente, lograr estos dos objetivos simultneamente no es feasible. Una forma segura de que un algoritmo de resource allocation aumente el throughput es permitir que tantos paquetes como sea posible entren en la red, para impulsar la utilización de todos los enlaces hasta el 100\%. El problema con esta estrategia es que aumentar el número de paquetes en la red también aumenta la longitud de las colas en cada enrutador. Las colas más largas, a su vez, significan que los paquetes se retrasan más tiempo en la red.

Para describir esta relación, algunos diseñadores de redes han propuesto han propuesto usar el ratio de throughput a delay como una métrica para evaluar la efectividad de un esquema de asignación de recursos. Este ratio es a veces referido como el poder de la red:

\begin{equation}
  \text{Power} = \frac{\text{Throughput}}{\text{Delay}} 
\end{equation}

El objetivo es maximizar este ratio, que es una función de cuánta carga tiene la red. Buscamos un sistema estable, es decir que los paquetes continúen pasando por la red incluso cuando la red está operando bajo una carga pesada. Si un mecanismo no es estable, la red puede experimentar un colapso de congestión.

\subsubsection*{Fair Resource Allocation}
Cuando varios flujos comparten un enlace en particular, nos gustaría que cada flujo reciba una parte igual del ancho de banda. Esta definición presume que una parte justa del ancho de banda significa una parte igual del ancho de banda. Pero muchas veces, partes iguales pueden no equivaler a partes justas.

Suponiendo que justo implica igual y que todos los caminos tienen la misma longitud, se propuso una métrica que se puede usar para cuantificar la equidad de un mecanismo de control de congestión. El índice de equidad de Jain se define de la siguiente manera: Dado un conjunto de throughput de flujos \((x_1, x_2, ..., x_n)\) (medido en unidades consistentes como bits / segundo), la siguiente función asigna un índice de equidad a los flujos:
\[
  f(x_1, x_2, ..., x_n) = \frac{(\sum_{i=1}^{n} x_i)^2}{n \sum_{i=1}^{n} x_i^2}
\]

El índice de equidad siempre da como resultado un número entre 0 y 1, siendo 1 la mayor equidad.

\subsubsection{Politicas de cola}
Independientemente de lo simple o sofisticado que sea el mecanismo de asignación de recursos, cada enrutador debe implementar una política de encolamiento que indique cómo se almacenan en búfer los paquetes mientras esperan ser transmitidos.

\subsubsection*{First In First Out (FIFO)}
El primer paquete que llega a un enrutador es el primer paquete en ser transmitido. Dado que la cantidad de espacio de búfer en cada enrutador es finita, si llega un paquete y la cola (espacio de búfer) está llena, el enrutador descarta ese paquete. Esto se hace sin tener en cuenta a qué flujo pertenece el paquete o qué tan importante es el paquete. A esto a veces se le llama \textbf{tail drop}, ya que los paquetes que llegan al final de la cola FIFO se descartan.

Una variación simple es usar encolamiento de prioridad. En esta politica se marca cada paquete con una prioridad; la marca podría llevarse, por ejemplo, en el encabezado IP. Los router implementan múltiples colas FIFO, una para cada clase de prioridad y siempre transmite paquetes desde la cola de mayor prioridad si esa cola no está vacía antes de pasar a la siguiente. Dentro de cada prioridad, los paquetes aún se administran de manera FIFO. Esta idea es una pequeña desviación del modelo de entrega de mejor esfuerzo, pero no va tan lejos como para garantizar una clase de prioridad en particular. 

\subsubsection*{Fair Queueing}
Se mantiene una cola separada por cada flujo que esté pasando por el router. El router envía paquetes desde esta cola usand round-robin. Cuando un flujo manda paquetes demasiado rápido, Cuando un flujo envía paquetes demasiado rápido, entonces su cola se llena. Cuando una cola alcanza su máxima longitud, los paquetes adicionales que pertenecen a ese flujo se descartan. De esta manera, una fuente determinada no puede aumentar arbitrariamente su parte de la capacidad de la red a expensas de otros flujos.

Sin embargo, los paquetes que se procesan en un enrutador no son necesariamente de la misma longitud. Para asignar verdaderamente el ancho de banda al enlace de salida de manera justa, es necesario tener en cuenta la longitud del paquete. Por esta razon, se usa un round robint bit a bit. Fair Queueing simula este comportamiento determinando primero cuándo terminaría de transmitirse un paquete determinado si se enviara utilizando este tipo de round robin y luego utilizando este tiempo de finalización para secuenciar los paquetes para su transmisión.


Sea \(P_i\) la longitud del paquete \(i\) de un flujo, sea \(S_i\) el tiempo en que el router comienza a transmitir el paquete \(i\) y sea \(F_i\) el tiempo en que el  termina de transmitirlo. Si \(P_i\) se expresa en términos de cuántos ticks de reloj se necesitan para transmitir ese paquete (teniendo en cuenta que el tiempo avanza 1 tick cada vez que este flujo obtiene 1 bit de servicio), entonces  \[F_i = S_i + P_i\]:
\begin{itemize}
  \item Si \(i\) llegó antes de que el router terminase de transmitir el paquete \(i-1\), entonces 
\end{itemize}




\newpage  
%Traducir:
Clearly, it is 
not feasible to interleave the bits from different packets. The FQ mecha-
nism therefore simulates this behavior by first determining when a given 
packet would finish being transmitted if it were being sent using bit-by-bit 
round-robin and then using this finishing time to sequence the packets 
for transmission.
% Al español:
Claramente, no es factible intercalar los bits de diferentes paquetes. .




% Resumen:

\red{
  
Introducción al problema de congestión. Curvas de Trafico Enviado vs entregado. Resultado con buffer infinit. Causas de congestión. Control de flujo vs Control de congestión. Taxonomia de Yang y Redan. Soluciones de lazo cerrado y abierto. Concepto de sistemas realimentados. Métricas a sensar para las realimentación. Realimentación implícita y explicita. Determinación de la performance.}